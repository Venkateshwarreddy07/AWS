WHAT IS EC2?

EC2 instance is a virtual server which makes things easier for the developer or business subscribers to run their program application in computing environment. You can launch as many virtual servers and configure security manage storage. These EC2 runs on EC2 hosts and this is availability zone service which is available within the AZ. If the AZ fails the EC2 goes down.

WHY EC2:

Provides Flexibility: EC2 instances can be customized with various configurations, including CPU, memory, storage, and networking options. Users have full control over instance size, operating system, software packages, and security settings, allowing them to tailor their infrastructure to their exact requirements.

Provides Scalability: EC2 allows users to compute capacity up or down based on their requirment. Features like autoscaling, which automatically adjusts the number of EC2 instances in response to changing traffic patterns or application load.

Cost Efficient: EC2 makes easy for users to built their infrastructue without financial constraints because it was built as pay as you go pricing where you need to pay for the things only you use and consume. 

High performance:EC2 offers a wide range of instance types optimized for different use cases and performance requirements. run on highly available and redundant hardware infrastructure within AWS data centers.

HOW ARE EC2 CREATED :

EC2 instances are created from the Amazon Machine Images, These AMI are the templates that are configred with an OS or any other software that determines user environment. Once an AMI is created and registered you can launch instances. AMI's can be shared to AWS accounts based on owner permissions. An instance can be launched using an AMI and an AMI can also be prepared from an instance.

We use various type of instances in different senarios lets have a glance:

TYPES OF INSTANCES:

GENERAL PURPOSE INSTANCE :

General purpose instances in Amazon EC2 are used for a wide range of workloads that require a balance of compute, memory, and networking resources. General-purpose instances are well-suited for hosting web applications, blogs. General-purpose instances are also commonly used for development and testing environments. Used to run Customer Relationalship Managment (CRM), Enterprise Resource Managment (ERP).

COMPUTE OPTIMIZED INSTANCE :
Compute Optimized Instances are used to run big data applications which require large amounts of processing power and memory in AWS cloud.These are mainly designed to run large applications that require fast network performance, High availability and high IOPS. Best example for this is data warehousing.

MEMORY OPTIMIZED INSTANCES :
Memory optimized instances are used mainly which requires large memory and fast access to memory data and less CPU power.Memory optimized instances are well-suited for hosting Java applications that require a large heap size to accommodate object memory allocation. They are mainly used for in-memory analytic platforms such as oracle ten times. 

STORAGE OPTIMIZED INSTANCES:

Storage-optimized instances are ideal for running big data analytics workloads that involve processing and analyzing large volumes of data stored in distributed file systems. Storage-optimized instances are commonly used for log and event processing which require storing, and analyzing large volumes of log data generated by applications.

GRAPHIC PROCESSING UNIT: 
GPU instances are used to perform graphic intensive applications fatser than standard EC2 instances. Mainly all the gamming applications reley on GPU instances.

MICRO :
Micro instance is used for applications with low throughput rate and can run very small application that require less transaction rates. The micro instance type can serve as a small database server, as a platform for software testing or as a web server that does not require high transaction rates.

Lets have a look into EC2 features, 

EC2 supports many operating systems, when EBS attached to EBS data is stored this can be attached and dettached flexibily and is persisitent storage where as instance store is non persistent, Amazon's Elastic IP service lets IP addresses be associated with an instance. Elastic IP addresses can be moved from instance to instance without requiring a network administrator's help,Amazon CloudWatch, This web service allows for the monitoring of AWS cloud services and the applications deployed on AWS.Automated scaling. Amazon EC2 Auto Scaling automatically adds or removes capacity from Amazon EC2 virtual servers in response to application demand.Pause and resume instances. EC2 instances can be paused and resumed from the same state later on. 

EC2 SECURITY:

EC2 is secured with security groups which is a firewall which controls inbound and outbound traffic. A private key is stored on local machine and a public key stored on EC2 instance and only if these key pair match and a user proves their identity a user can have access to the EC2.

EC2 INSTANCE NAMING :  R7dn.6*large    R- Instance Family, 7- Instance Generation, dn- Additional Capabilities, 6*large- Instance Size. 


VIRTUALIZATION: 

Virtualization is a process of running more than one O.S with multiple applications on a single hardware or single server which is very effective. This process of functioning multiple O.S with different application can be done using hypervisor. A server with its O.S as such multiple servers with their different O.S are functioned to be run on a single hypervisor.

HYPERVISOR: A hypervisor is a layer that enables multiple operating systems or virtual machines  to run on a single physical machine or server hardware. 

STORAGE REFRESHER : 

DIRECT ATTACHED STORAGE :  This is the storage on EC2 and any obstruction caused to the EC2 looses all the data.
NETWORK ATTACHED STORAGE : Volumes are created and attached over network(EBS) highly resistent and seperate from instance hardware storage.
EPHEMERAL STORAGE: This is a temporary storage that donot exist over a long period of time.
PERSISTENT STORAGE : This storage lives past the lifetime of a instance.
BLOCK STORAGE : In the volume provided blocks are stored and arranged randomly without any structure and upon thid OS is placed which converts into file storage.

FILE STORAGE : Presented as file share and this has structure and is mountable.
OBJECT STORAGE : This has no structure, is collection of objects i.e it can be of anything like movies, pictures,etc its a flat thing has no structure.

ELASTIC BLOCK STORAGE: This is a storage service in which raw data is divided and stored in form of disks and these disks are allocated with block ID and these are organized and stored this is known as volumes. Generally EBS are connected to instance where all the data that functioned on instance will be stored in these volumes and for data availability and data recovery these EBS volumes are connect to S3 buckets where all the data is persistently stored in S3 buckets In S3 all the data is stored permanently and you can also take the snapshot of the data stored in S3 from EBS. Even if the instance goes down the data can be retrieved from the EBS. If you dettach EBS and attach it to other instance all the data in this EBS will be available and can be functioned on the newly attached instace.

These EBS can be detached and can be reattached to other instance so that all the data available in this EBS can be functioned on the newly connected instance this can also be done across different regions.

EBS VOLUME TYPES:  

GENERAL PURPOSE SSD -GP2 : Used in case of low latency, boot volumes, interactive applications, developer and testing environments.
It can be 1GB(MIN) -  16TB(MAX). When you create it, It is created with 1IO credit allocation ( 1IO credit = 16kb of data).Designed for broad range of workloads including small to medium sized databases used in developing and testing environments. GP2 volumes deliver a baseline performance of 3 IOPS per GB, with ability to burst upto 3000 IOPS for short periods. It is designed to deliver consistent throughput of 128 mb/s per terabyte.Suitable for most general purpose workloads where balanced performance is requried.

GENERAL PURPOSE SSD - GP3 : Offers high performance compared to GP2. Baseline performance is 3000 IOPS with ability to burst upto 16000 IOPS for short periods. GP3 volume provide baseline throughput og 125mb/s per terabyte. Recommended for wide variety of transactional and analytical workloads where cost effectiveness and flexibility are important.

PROVISIONAL IOPS SSD (IO 1/2) :
These IOPS volumes are designed for applications that requie high performance, low latency,consistent IOPS performance. Io1 volume support upto 64000 IOPS, while IO 2 supports upto 64000iops for every 1TB of storage.Maximum throughput of 1000mb/s for io 1 and 8000b/s for io2. Ideal for critical business application, large relational databases, other high performances.

HARD DISK DRIVE (ST1) : They are optimized for large streaming workloads where throughput is critical. ST1 volumes are less focused on IOPS and more on delivering high throughput. These volumes are optimized for streaming workloads with a consistent baseline throughput of 40mb/s terabyte upto max throughput of 500mb/s. Suitable for bigdata processing, datawarehouse, log processing and applications that require high consistent throughput.

COLD HARD DISK DRIVE (SC1) : SC1 volumes are designed for infrequently accessed throught, intensive workloads where cost savings are primary considerations. They provide lower cost storage for senarios where performance is less critical. Similar to st1, sc1 are more focused on throughput and offers low level of IOPS relative to their size. Provide baseline throughput of 12mb/s terabyte of volume size, upto maximum throughput of 250 mb/s. Ideal for senario with large datasets and infrequent access such as backup storage, data archives.


INSTANT STORE VOLUMES : This is a type of storagw wgere the data is stored temporarily and can be said as Non-persistent data storage. Data can be lost due to hardware failure or instance rebooting or any as such and this is a temporary type storage.This volume type is choosen when data needs to be stored just for that instance and no need of storing data for longer use.These provide block storage devices that can be connected to instance presented to OS and used as bases for file systems which can be used for application. These are physically connect to one EC2 host, each EC2 has its own instance store volume that isolated to one particular host.


SNAPSHOTS: 

Snapshots These are defined as the data replica or data copies from an S3. All the data that stored in an S3 and die to some technical issues the instance goes down and if the needs To be retrieved then we can take the snapshots from the S3 bucket that is connected to the EBSand this snapshots are used to create a new EBS and this EBS can be connect to different instance in different region. This acts as a back data copy of a function. 

Lets assume for a instance attached with EBS to S3 had functioned from a period of time T1 - T2 and the data stored during this time period is D1 now if you take a snapshot you will get the snapshor of D1. Now you ran the instance from time period T2 - T3 and the data stored during this time period is D2 and now if you take a snapshot you will get the snapshot as D2 where you donot find any D1 data prints. During second snapshot it only gives the changes that happened to D1. where all data D=D1+D2

BOOTSTRAPING:

What is Bootstrapping ?

Bootstrapping refers to the process of automatically configuring and provisioning resources or instances when they are launched. It involves executing scripts or commands on an instance during its initialization phase to set up the environment, install software, configure settings, and perform other tasks required to prepare the instance for its intended purpose. Bootstrapping is commonly used in cloud computing environments, including AWS, to automate the setup and configuration of instances, ensuring consistency, repeatability, and efficiency. It allows users to define the desired state of an instance or environment through scripts or configuration files, which are executed during the bootstrapping process. This is a launch time configuration will be initiated and executed only during the launch time and will not be executed if the user data is updated and the restart will be done only once for pre configuring. 

User data is executed only once during the post launch of the instance. User data is just meta data and opaque to the EC2 and is not secure and not recommended to enter security credentials or any confidential information.  User data is limited unto 16KB, It can be updated by stopping the instance but cannot be executed again its only done during the launch of the instance.


WHY BOOTSTRAPING :

Generally when you launch an instance manually it takes certain time to create and run the instance and this time is known as post launch time and its might take few minutes and inorder to reduce this time we use bootstrapping which reduces post launch time by pre-configuring and automation.  You can also pre-bake the AMI where you can alternatively do the work in this method you can front load the work in AMI before launching the instance and creating the AMI by all the work baked in. Now we can combine both the process AMI- baking and bootstrapping to  reduce the post launch time.

Generally lets say we need to run an Wordpress application on an instance for this we need to create a data base and give an html file and all these need to be done in aws cli and running all these commands individually is time consuming so we enter all the commands as user meta data while creating the instance and launch the instance and when you click the ip address of the instance you will be directed to the Wordpress web page which is time saving. Here we do not need to enter each and every command individually and we can enter all the commands as user meta data and the instance processes and will launch the instance based on the user meta data.  This process can also be done using CloudFormation which is infrastructure as a code where you write all the code based on the application requirements in a stack and the CloudFormation will process then rest based on the given stack.


EC2 Instance Roles : Instance role is a specific type of IAM role designed so that it can be assumed by an ec2 instance. When a role is created in IAM and the permissions are also assigned to that particular role and in parallel an instance profile is also created with that particular name and this allows the credentials into the instance. You can create the IAM roles and modify them by attaching them to ec2 and evoke them using cli.

AWS Systems Manager Parameter Role: A service from aws that makes easy to store various bits of system configuration. Strings, secrets, and bits store them in secure and resilient way. In the above discussion we have discussed that in user data we cannot use security credentials because anyone having access to that instance can have the credentials so here we can overcome this issue by systems manager parameter. Parameter store lets you create a parameter these parameter have parameter name and parameter value and value is the part that stores actual configuration , Many aws services integrate with the parameter store natively.

Parameter store offers to store three different types of parameters:   1) String 2) Stringlist 3) SecureString and using these three parameters you can store things inside the product such as license codes, database strings, configs and passwords. Parameter store also stores different versions of object same as object versioning in s3.


AWS Placement Groups :

A placement group is a logical grouping of instances within the same Availability Zone (AZ) that enables users to influence the placement of instances to meet specific requirements for performance, latency, or high availability. Placement groups allow users to control how instances are placed relative to each other within the same AZ, which can impact factors such as network latency, throughput, and fault tolerance.

There are three types of placement groups in AWS:

Cluster Placement Group:

A cluster placement group is designed for applications that require low-latency, high-throughput networking between instances.
Instances in a cluster placement group are placed in close proximity to each other within a single AZ.
This placement strategy minimizes the network latency and enables high-bandwidth communication between instances, making it suitable for tightly-coupled, high-performance computing (HPC) workloads, such as scientific simulations, financial modeling, and grid computing.

Partition Placement Group:

A partition placement group is suitable for distributing instances across logical partitions within a single AZ.
Each partition acts as an independent rack within the AZ, with its own set of resources such as power source, network switches, and network cables.
Partition placement groups are recommended for applications that require a high degree of fault tolerance and availability, as they provide isolation at the rack level.

Spread Placement Group:

A spread placement group ensures that instances are placed on distinct underlying hardware infrastructure within a single AZ.
Spread placement groups are designed to provide a high level of fault tolerance by spreading instances across multiple racks, ensuring that they are isolated from each other.
This placement strategy is suitable for applications that require a high degree of resilience and availability, such as stateful applications or databases.

Placement groups have certain limitations and considerations:

Each AWS account has limits on the number of placement groups per Region, as well as the number of instances per placement group.
Once an instance is launched in a placement group, it cannot be moved to another placement group.
The choice of placement group type depends on the specific requirements of the application, such as performance, fault tolerance, and availability.
Not all instance types are supported in placement groups, so users should consult the AWS documentation for compatibility information.
In summary, placement groups in AWS provide users with fine-grained control over the placement of instances within a single AZ, allowing them to optimize performance, latency, and fault tolerance for their applications. By choosing the appropriate placement group type, users can ensure that their workloads meet the desired requirements for performance, availability, and resilience. 

***********************************************************************************************************************************************************************************

VPC

VPC is a virtual private network I.e a logically isolated section which is defined by a user in a cloud. This enables you to create a secure and scalable environment for running your applications and services in the cloud while maintaining isolation from other AWS customers. 

This acts like a plot in a vast area where you can built your boundaries, create your applications create your own IP address range and can have complete control over incoming and outgoing traffic, create subnets organize resources and setup your own network environment in the way you wanted. In other words its like building your own empire in the way you wanted creating your security groups and creating the things you wanted in the cloud. Here the empire is compared to the application you need to build in a VPC.

VPC is a availability zone service it is acessible acrozz different availability zones but a VPC created in one region is not available in other region and if a AZ goes down tjhe vpc in that AZ also goes down.

VPC ROUTER :
This VPC router is responsible for routing network inbound and outbound traffic between different subnets within the VPC. This contains routing table with IP addrfesses and the router controls the traffic based on these routing tables.

INTERNET GATEWAY:
The internet gateway acts as bridge connecting the VPC to the internet resources. This acts as a entry and exit point for the network traffic between VPC and internet.IGW provides connectivity between a VPC and the public internet, enabling instances within the VPC to access the internet and allowing external users or services to communicate with instances in the VPC
With VPC we can create security groups and access control lists to control netwrok inbound and outbound traffic effectively creating a secure environment.

A Virtual Private Cloud (VPC) is a private network within a public cloud environment, such as Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure. It provides a logically isolated section of the cloud where users can launch resources in a virtual network that they define.

Key Features:
Isolation: Ensures that resources within a VPC are isolated from other resources in the public cloud.
Customization: Allows the configuration of IP address ranges, creation of subnets, and setting up route tables and network gateways.
Security: Provides robust security features, including security groups, network access control lists (ACLs), and the ability to create private subnets.
Uses of VPC:
Secure Environment for Applications: Run applications in an isolated environment, ensuring security and compliance.
Hybrid Cloud Architectures: Extend on-premises networks into the cloud, creating a hybrid cloud environment.
Network Control: Manage and control network configurations, including IP addresses, subnets, and routing policies.
Data Security: Protect sensitive data by hosting it in private subnets and controlling access with security policies.
Real-Time Use Cases:
E-Commerce Platforms: An e-commerce company can use a VPC to host its web application, databases, and payment processing systems in a secure and scalable manner.
Financial Services: Banks and financial institutions can deploy their applications within a VPC to ensure data privacy and meet regulatory requirements.
Healthcare Systems: Healthcare providers can use VPCs to store and manage patient data securely, complying with regulations like HIPAA.
Enterprise Applications: Large enterprises can migrate their legacy applications to the cloud within a VPC to leverage cloud scalability while maintaining network isolation.
Development and Testing: Development teams can create isolated environments for developing and testing applications without affecting the production environment.
Overall, VPCs provide a versatile and secure way to manage cloud resources, making them essential for businesses looking to leverage cloud computing while maintaining control over their network environment.

***********************************************************************************************************************************************************************************

S3- SIMPLE STORAGE SERVICE:
Amazon S3 (Simple Storage Service) buckets are containers for storing objects in the Amazon Web Services (AWS) cloud. They are essentially folders within the S3 service where you can store and organize data, such as documents, images, videos, and application data.

WHY S3 :

S3 generally used for storing any kind of data. S3 bucket is highly available storage service used to store wide range of data types. Many organizations use S3 to store,backup,retrive data securly from the cloud.S3 buckets are used foe databackup and store critical data for longterm with snapshots as alternate security to overcome disaster recovery, dataloss.S3 is private by default.

S3 ACCESS: 

When we create a S3 bucket a bucket can be managed by set of rules here they are policies lets look into them, 

IDENTITY BASED POLICIES:  These are directly attached to IAM (Identity and Access Management) users, groups, or roles. These policies define what actions a specific user can perform on Amazon S3 resources. You are an admin and have read only acess and developer may have both the read and write access this actions are controlled by IAM policies.

RESOURCE-BASED POLICIES: These are attached to S3 buckets or objects. These policies define who can access the resources within the bucket or object and what actions they can perform. In this policies we define the cross region access as S3 being the regional service. Lets say there is a admin and developer admin may not have access to developer code this is what the resource policies deals with the access of letting the roles have permission or not.

INLINE POLOCIES : These are said to all the additional polocies that are created by the owner to customize and create his individual set of rules.
ACL: 


STATIC WEBSITE HOSTING:  

We generally access S3 objects using AWS API's but using static website hosting makes it more secure because it allows access through standard https by individuals using web browser.

While hosting a static website we enable index and error documents these documents come  into action when you try to access a unspecified page you will get the index document for example you searched amazon you will get a amazon main index page because you didn't specify what you want in amazon. Now when you try to access a anything that you are restricted or anything that doesn't exist you will receive and error page.

Now if you host a static website by enabling static website hosting you can host a website but still you cannot access the website because S3 is always private inorderr to access the website you need to add the bucket policy making the website publicly accessible to everyone.  

VERSIONING:


In S3 bucket versioning it is a feature in S3 which allows to store multiple versions of an object in a bucket without overriding and it gives you the latest version keeping all the previous versions archived but you still have access to those previous versions. You can enable versioning from disable to enable but can't disable it again you can suspend and reenable it but cannot disable. .Versioning is controlled at the bucket level. 

Now if you enable versioning and upload an object the object is assigned with a Version ID and now lets say we have uploaded an object A and A is assigned with an object ID now you want to make some changes to A so you upload a new object A with some additions changes now the new A is assigned with other ID and this one will be available in the bucket but still you have the older version with old version id in the bucket and can also have access to that object. this is where you will have the older versions and can avoid overriding. the one which you uploaded recently is the current version running in the bucket.

In versioning lets assume you have uploaded A and B now if you delete A the deleted object A is stored as deleted mark A in the bucket but not deleted where you can still have access to the deleted one it is just marked a deleted but not deleted.If you want to delete it permananelty you need to select the deleted marked object enter the version ID and then delete permanently.

PERFORMANCE OPTIMIZATION : We generally upload objects into S3 buckets lets assume we are uploading an object consists of data 7GB and it had uploaded unto 6.5 GB and due to some technical issues the upload fails the only way to continue this is the reuploasd from starting which causes the data loss and is a time consuming to improve this performance they came up with a solution the same 6GB file it split into many sections and these section can contain from 5MB to 5GB and all these sections transfer the data paralally at same time and if the one section fails the particular section starts reuploading from beginning and the data is divided into section each section takes less time to make a reupload. this process is known as performance optimization. 


KEY MANAGEMENT SYSTEM:

KMS the name itself suggests  managing keys. It lets you create, store and manage cryptographic keys, these are the keys used to convert plain text ti cipher text and vice versa. It is capable of handling both symmetric and asymmetric keys, It can also perform encryption and decryption operations. KMS also provides a FIPS 140-2 level 2 security service this is a US security standard. KMS key contain key ID, creation date, key policy, key description and state of key. Every KMS key is backed by physical key material this is used to encrypt and decrypt things that you give. This key material is imported or created in KMS and this is used to encrypt or decrypt data unto 4KB in size. KMS generally only used to work on small bits of data. when a key is created in KMS it is created with physical backing material and is stored in encrypted form nothing in KMS is stored in plain text format.  Now the user makes an encrypt call to KMS the KMS verifies the users have the permission to KMS and now the encrypted key will be decrypted and KMS accepts and the plain text sent by the user will be encrypted with this keys and return to the user. Whenever the user wants to decrypt this data the user send the decrypt call and send the encrypted data, here the decryption process is different all the data required to decrypt the encrypted key is available in itself. Now the encrypted key is accepted and will be decrypted and sent to user. In this whole process the keys never leave the KMS system and data is never stored in plaintext form at any point in KMS. DEK's are the data encryption keys which are another type of keys that KMS can generate to encrypt data that is larger than 4KB. KMS doesn't store the encrypted or decrypted keys it just creates them using the operations and will discard them after use but never restores them.  when a data encryption key is generated the KMS provides two versions one is plain text where an immediate cryptographic action is performed and then this encrypted key is encrypted using KMS.



S3 REPLICATION: S3 replication is a feature in S3 buckets which automatitally copies objects from one amazon S3 bucket to other bucket in same or different region. This enables you to replicate objects in buckets for various reasons.

Explain S3 replication and mention its uses it is used for various reasons such as data availability and disaster recovery.

PRE ASSIGNED URLs:

where you can give other person or application access to an object inside a bucket using your credentials in a safe and secured way.

Now upload an object into a bucket and now you can generate pre assigned URL using cloud shell and also in console. while generating url you need to assign time till which the URL is valid. now you can have access to that object using this url and now if you add an inline policy that restricts the access of iamadmin now you do not have access to the object but still you can create preassigned URL but this url shows the access denied. This is mainly used in remote work scenarios. 

***********************************************************************************************************************************************************************************


 
