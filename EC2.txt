WHAT IS EC2?

EC2 instance is a virtual server which makes things easier for the developer or business subscribers to run their program application in computing environment. You can launch as many virtual servers and configure security manage storage. These EC2 runs on EC2 hosts and this is availability zone service which is available within the AZ. If the AZ fails the EC2 goes down.

WHY EC2:

Provides Flexibility: EC2 instances can be customized with various configurations, including CPU, memory, storage, and networking options. Users have full control over instance size, operating system, software packages, and security settings, allowing them to tailor their infrastructure to their exact requirements.

Provides Scalability: EC2 allows users to compute capacity up or down based on their requirment. Features like autoscaling, which automatically adjusts the number of EC2 instances in response to changing traffic patterns or application load.

Cost Efficient: EC2 makes easy for users to built their infrastructue without financial constraints because it was built as pay as you go pricing where you need to pay for the things only you use and consume. 

High performance:EC2 offers a wide range of instance types optimized for different use cases and performance requirements. run on highly available and redundant hardware infrastructure within AWS data centers.

HOW ARE EC2 CREATED :

EC2 instances are created from the Amazon Machine Images, These AMI are the templates that are configred with an OS or any other software that determines user environment. Once an AMI is created and registered you can launch instances. AMI's can be shared to AWS accounts based on owner permissions. An instance can be launched using an AMI and an AMI can also be prepared from an instance.

We use various type of instances in different senarios lets have a glance:

TYPES OF INSTANCES:

GENERAL PURPOSE INSTANCE :

General purpose instances in Amazon EC2 are used for a wide range of workloads that require a balance of compute, memory, and networking resources. General-purpose instances are well-suited for hosting web applications, blogs. General-purpose instances are also commonly used for development and testing environments. Used to run Customer Relationalship Managment (CRM), Enterprise Resource Managment (ERP).

COMPUTE OPTIMIZED INSTANCE :
Compute Optimized Instances are used to run big data applications which require large amounts of processing power and memory in AWS cloud.These are mainly designed to run large applications that require fast network performance, High availability and high IOPS. Best example for this is data warehousing.

MEMORY OPTIMIZED INSTANCES :
Memory optimized instances are used mainly which requires large memory and fast access to memory data and less CPU power.Memory optimized instances are well-suited for hosting Java applications that require a large heap size to accommodate object memory allocation. They are mainly used for in-memory analytic platforms such as oracle ten times. 

STORAGE OPTIMIZED INSTANCES:

Storage-optimized instances are ideal for running big data analytics workloads that involve processing and analyzing large volumes of data stored in distributed file systems. Storage-optimized instances are commonly used for log and event processing which require storing, and analyzing large volumes of log data generated by applications.

GRAPHIC PROCESSING UNIT: 
GPU instances are used to perform graphic intensive applications fatser than standard EC2 instances. Mainly all the gamming applications reley on GPU instances.

MICRO :
Micro instance is used for applications with low throughput rate and can run very small application that require less transaction rates. The micro instance type can serve as a small database server, as a platform for software testing or as a web server that does not require high transaction rates.

Lets have a look into EC2 features, 

EC2 supports many operating systems, when EBS attached to EBS data is stored this can be attached and dettached flexibily and is persisitent storage where as instance store is non persistent, Amazon's Elastic IP service lets IP addresses be associated with an instance. Elastic IP addresses can be moved from instance to instance without requiring a network administrator's help,Amazon CloudWatch, This web service allows for the monitoring of AWS cloud services and the applications deployed on AWS.Automated scaling. Amazon EC2 Auto Scaling automatically adds or removes capacity from Amazon EC2 virtual servers in response to application demand.Pause and resume instances. EC2 instances can be paused and resumed from the same state later on. 

EC2 SECURITY:

EC2 is secured with security groups which is a firewall which controls inbound and outbound traffic. A private key is stored on local machine and a public key stored on EC2 instance and only if these key pair match and a user proves their identity a user can have access to the EC2.

EC2 INSTANCE NAMING :  R7dn.6*large    R- Instance Family, 7- Instance Generation, dn- Additional Capabilities, 6*large- Instance Size. 


VIRTUALIZATION: 

Virtualization is a process of running more than one O.S with multiple applications on a single hardware or single server which is very effective. This process of functioning multiple O.S with different application can be done using hypervisor. A server with its O.S as such multiple servers with their different O.S are functioned to be run on a single hypervisor.

HYPERVISOR: A hypervisor is a layer that enables multiple operating systems or virtual machines  to run on a single physical machine or server hardware. 

STORAGE REFRESHER : 

DIRECT ATTACHED STORAGE :  This is the storage on EC2 and any obstruction caused to the EC2 looses all the data.
NETWORK ATTACHED STORAGE : Volumes are created and attached over network(EBS) highly resistent and seperate from instance hardware storage.
EPHEMERAL STORAGE: This is a temporary storage that donot exist over a long period of time.
PERSISTENT STORAGE : This storage lives past the lifetime of a instance.
BLOCK STORAGE : In the volume provided blocks are stored and arranged randomly without any structure and upon thid OS is placed which converts into file storage.

FILE STORAGE : Presented as file share and this has structure and is mountable.
OBJECT STORAGE : This has no structure, is collection of objects i.e it can be of anything like movies, pictures,etc its a flat thing has no structure.

ELASTIC BLOCK STORAGE: This is a storage service in which raw data is divided and stored in form of disks and these disks are allocated with block ID and these are organized and stored this is known as volumes. Generally EBS are connected to instance where all the data that functioned on instance will be stored in these volumes and for data availability and data recovery these EBS volumes are connect to S3 buckets where all the data is persistently stored in S3 buckets In S3 all the data is stored permanently and you can also take the snapshot of the data stored in S3 from EBS. Even if the instance goes down the data can be retrieved from the EBS. If you dettach EBS and attach it to other instance all the data in this EBS will be available and can be functioned on the newly attached instace.

These EBS can be detached and can be reattached to other instance so that all the data available in this EBS can be functioned on the newly connected instance this can also be done across different regions.

EBS VOLUME TYPES:  

GENERAL PURPOSE SSD -GP2 : Used in case of low latency, boot volumes, interactive applications, developer and testing environments.
It can be 1GB(MIN) -  16TB(MAX). When you create it, It is created with 1IO credit allocation ( 1IO credit = 16kb of data).Designed for broad range of workloads including small to medium sized databases used in developing and testing environments. GP2 volumes deliver a baseline performance of 3 IOPS per GB, with ability to burst upto 3000 IOPS for short periods. It is designed to deliver consistent throughput of 128 mb/s per terabyte.Suitable for most general purpose workloads where balanced performance is requried.

GENERAL PURPOSE SSD - GP3 : Offers high performance compared to GP2. Baseline performance is 3000 IOPS with ability to burst upto 16000 IOPS for short periods. GP3 volume provide baseline throughput og 125mb/s per terabyte. Recommended for wide variety of transactional and analytical workloads where cost effectiveness and flexibility are important.

PROVISIONAL IOPS SSD (IO 1/2) :
These IOPS volumes are designed for applications that requie high performance, low latency,consistent IOPS performance. Io1 volume support upto 64000 IOPS, while IO 2 supports upto 64000iops for every 1TB of storage.Maximum throughput of 1000mb/s for io 1 and 8000b/s for io2. Ideal for critical business application, large relational databases, other high performances.

HARD DISK DRIVE (ST1) : They are optimized for large streaming workloads where throughput is critical. ST1 volumes are less focused on IOPS and more on delivering high throughput. These volumes are optimized for streaming workloads with a consistent baseline throughput of 40mb/s terabyte upto max throughput of 500mb/s. Suitable for bigdata processing, datawarehouse, log processing and applications that require high consistent throughput.

COLD HARD DISK DRIVE (SC1) : SC1 volumes are designed for infrequently accessed throught, intensive workloads where cost savings are primary considerations. They provide lower cost storage for senarios where performance is less critical. Similar to st1, sc1 are more focused on throughput and offers low level of IOPS relative to their size. Provide baseline throughput of 12mb/s terabyte of volume size, upto maximum throughput of 250 mb/s. Ideal for senario with large datasets and infrequent access such as backup storage, data archives.


INSTANT STORE VOLUMES : This is a type of storagw wgere the data is stored temporarily and can be said as Non-persistent data storage. Data can be lost due to hardware failure or instance rebooting or any as such and this is a temporary type storage.This volume type is choosen when data needs to be stored just for that instance and no need of storing data for longer use.These provide block storage devices that can be connected to instance presented to OS and used as bases for file systems which can be used for application. These are physically connect to one EC2 host, each EC2 has its own instance store volume that isolated to one particular host.


SNAPSHOTS: 

Snapshots These are defined as the data replica or data copies from an S3. All the data that stored in an S3 and die to some technical issues the instance goes down and if the needs To be retrieved then we can take the snapshots from the S3 bucket that is connected to the EBSand this snapshots are used to create a new EBS and this EBS can be connect to different instance in different region. This acts as a back data copy of a function. 

Lets assume for a instance attached with EBS to S3 had functioned from a period of time T1 - T2 and the data stored during this time period is D1 now if you take a snapshot you will get the snapshor of D1. Now you ran the instance from time period T2 - T3 and the data stored during this time period is D2 and now if you take a snapshot you will get the snapshot as D2 where you donot find any D1 data prints. During second snapshot it only gives the changes that happened to D1. where all data D=D1+D2

BOOTSTRAPING:

What is Bootstrapping ?

Bootstrapping refers to the process of automatically configuring and provisioning resources or instances when they are launched. It involves executing scripts or commands on an instance during its initialization phase to set up the environment, install software, configure settings, and perform other tasks required to prepare the instance for its intended purpose. Bootstrapping is commonly used in cloud computing environments, including AWS, to automate the setup and configuration of instances, ensuring consistency, repeatability, and efficiency. It allows users to define the desired state of an instance or environment through scripts or configuration files, which are executed during the bootstrapping process. This is a launch time configuration will be initiated and executed only during the launch time and will not be executed if the user data is updated and the restart will be done only once for pre configuring. 

User data is executed only once during the post launch of the instance. User data is just meta data and opaque to the EC2 and is not secure and not recommended to enter security credentials or any confidential information.  User data is limited unto 16KB, It can be updated by stopping the instance but cannot be executed again its only done during the launch of the instance.


WHY BOOTSTRAPING :

Generally when you launch an instance manually it takes certain time to create and run the instance and this time is known as post launch time and its might take few minutes and inorder to reduce this time we use bootstrapping which reduces post launch time by pre-configuring and automation.  You can also pre-bake the AMI where you can alternatively do the work in this method you can front load the work in AMI before launching the instance and creating the AMI by all the work baked in. Now we can combine both the process AMI- baking and bootstrapping to  reduce the post launch time.

Generally lets say we need to run an Wordpress application on an instance for this we need to create a data base and give an html file and all these need to be done in aws cli and running all these commands individually is time consuming so we enter all the commands as user meta data while creating the instance and launch the instance and when you click the ip address of the instance you will be directed to the Wordpress web page which is time saving. Here we do not need to enter each and every command individually and we can enter all the commands as user meta data and the instance processes and will launch the instance based on the user meta data.  This process can also be done using CloudFormation which is infrastructure as a code where you write all the code based on the application requirements in a stack and the CloudFormation will process then rest based on the given stack.


EC2 Instance Roles : Instance role is a specific type of IAM role designed so that it can be assumed by an ec2 instance. When a role is created in IAM and the permissions are also assigned to that particular role and in parallel an instance profile is also created with that particular name and this allows the credentials into the instance. You can create the IAM roles and modify them by attaching them to ec2 and evoke them using cli.

AWS Systems Manager Parameter Role: A service from aws that makes easy to store various bits of system configuration. Strings, secrets, and bits store them in secure and resilient way. In the above discussion we have discussed that in user data we cannot use security credentials because anyone having access to that instance can have the credentials so here we can overcome this issue by systems manager parameter. Parameter store lets you create a parameter these parameter have parameter name and parameter value and value is the part that stores actual configuration , Many aws services integrate with the parameter store natively.

Parameter store offers to store three different types of parameters:   1) String 2) Stringlist 3) SecureString and using these three parameters you can store things inside the product such as license codes, database strings, configs and passwords. Parameter store also stores different versions of object same as object versioning in s3.


AWS Placement Groups :

A placement group is a logical grouping of instances within the same Availability Zone (AZ) that enables users to influence the placement of instances to meet specific requirements for performance, latency, or high availability. Placement groups allow users to control how instances are placed relative to each other within the same AZ, which can impact factors such as network latency, throughput, and fault tolerance.

There are three types of placement groups in AWS:

Cluster Placement Group:

A cluster placement group is designed for applications that require low-latency, high-throughput networking between instances.
Instances in a cluster placement group are placed in close proximity to each other within a single AZ.
This placement strategy minimizes the network latency and enables high-bandwidth communication between instances, making it suitable for tightly-coupled, high-performance computing (HPC) workloads, such as scientific simulations, financial modeling, and grid computing.

Partition Placement Group:

A partition placement group is suitable for distributing instances across logical partitions within a single AZ.
Each partition acts as an independent rack within the AZ, with its own set of resources such as power source, network switches, and network cables.
Partition placement groups are recommended for applications that require a high degree of fault tolerance and availability, as they provide isolation at the rack level.

Spread Placement Group:

A spread placement group ensures that instances are placed on distinct underlying hardware infrastructure within a single AZ.
Spread placement groups are designed to provide a high level of fault tolerance by spreading instances across multiple racks, ensuring that they are isolated from each other.
This placement strategy is suitable for applications that require a high degree of resilience and availability, such as stateful applications or databases.

Placement groups have certain limitations and considerations:

Each AWS account has limits on the number of placement groups per Region, as well as the number of instances per placement group.
Once an instance is launched in a placement group, it cannot be moved to another placement group.
The choice of placement group type depends on the specific requirements of the application, such as performance, fault tolerance, and availability.
Not all instance types are supported in placement groups, so users should consult the AWS documentation for compatibility information.
In summary, placement groups in AWS provide users with fine-grained control over the placement of instances within a single AZ, allowing them to optimize performance, latency, and fault tolerance for their applications. By choosing the appropriate placement group type, users can ensure that their workloads meet the desired requirements for performance, availability, and resilience. 






 
